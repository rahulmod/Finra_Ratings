from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import time
import pandas as pd

# Set up Chrome options
chrome_options = Options()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--window-size=1920,1080")

# Set up the Chrome driver
# https://storage.googleapis.com/chrome-for-testing-public/126.0.6478.185/win64/chromedriver-win64.zip
service = Service('F:/chromedriver-126/chromedriver.exe')  # Update this path
driver = webdriver.Chrome(service=service, options=chrome_options)

# Navigate to the page
url = "https://www.finra.org/finra-data/fixed-income/corp-and-agency"
driver.get(url)


# Function to check if element is present
def is_element_present(driver, by, value, timeout=10):
    try:
        WebDriverWait(driver, timeout).until(EC.presence_of_element_located((by, value)))
        return True
    except TimeoutException:
        return False


# Wait for the page to load and handle potential popups
wait = WebDriverWait(driver, 30)
try:
    # Check for and close any popups or cookie consent
    if is_element_present(driver, By.ID, "onetrust-accept-btn-handler"):
        driver.find_element(By.ID, "onetrust-accept-btn-handler").click()

    # Wait for the grid to load, trying different possible class names
    grid = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".ag-body-viewport, .ag-center-cols-container")))
except TimeoutException:
    print("Timed out waiting for page to load")
    driver.quit()
    exit(1)


# Function to scrape visible rows
def scrape_visible_rows():
    rows = driver.find_elements(By.CSS_SELECTOR, ".ag-body-viewport .ag-row, .ag-center-cols-container .ag-row")
    data = []
    for row in rows:
        cells = row.find_elements(By.CSS_SELECTOR, ".ag-cell")
        row_data = [cell.text for cell in cells]
        if any(row_data):  # Only add non-empty rows
            data.append(row_data)
    return data


# Scrape data
all_data = []
last_row_count = 0
attempts = 0
max_attempts = 5

while attempts < max_attempts:
    # Scrape visible rows
    data = scrape_visible_rows()
    all_data.extend(data)

    # Check if we've reached the end
    if len(all_data) == last_row_count:
        attempts += 1
    else:
        attempts = 0

    last_row_count = len(all_data)

    # Scroll to the bottom of the grid
    driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", grid)
    time.sleep(2)  # Increased wait time

# Close the browser
driver.quit()

# Create a DataFrame
columns = ["CUSIP", "Issuer Name", "Coupon", "Maturity", "Last Update Date"]
df = pd.DataFrame(all_data, columns=columns)

# Save to CSV
df.to_csv("finra_corp_and_agency_data.csv", index=False)

print(f"Scraped {len(df)} rows of data and saved to finra_corp_and_agency_data.csv")
